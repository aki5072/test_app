import requests
import json
import os
import time
import html2text
import re
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# --- è¨­å®š ---
USER_ID = "genel"
OUTPUT_DIR = f"./{USER_ID}_articles"

def sanitize_filename(title):
    """ãƒ•ã‚¡ã‚¤ãƒ«åã¨ã—ã¦å®‰å…¨ãªæ–‡å­—åˆ—ã«å¤‰æ›ã™ã‚‹"""
    # è‹±æ•°å­—ã€æ—¥æœ¬èªï¼ˆã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ï¼‰ã€ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã€ãƒã‚¤ãƒ•ãƒ³ä»¥å¤–ã®æ–‡å­—ã‚’ã™ã¹ã¦ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã«ç½®æ›
    cleaned_title = re.sub(r'[^a-zA-Z0-9_\-\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF]+', "_", title)
    cleaned_title = re.sub(r'__+', "_", cleaned_title)
    cleaned_title = cleaned_title.strip("_")
    return cleaned_title

def get_all_notes_info(user_id):
    """æŒ‡å®šã•ã‚ŒãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¨è¨˜äº‹ã‚­ãƒ¼ã¨ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã®æƒ…å ±ã‚’å–å¾—ã—ã¾ã™ã€‚"""
    all_notes = {}
    page = 1
    print("å…¨è¨˜äº‹ã®åŸºæœ¬æƒ…å ±ï¼ˆã‚­ãƒ¼ã€ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ï¼‰ã®å–å¾—ã‚’é–‹å§‹ã—ã¾ã™...")
    while True:
        try:
            print(f"{page}ãƒšãƒ¼ã‚¸ç›®ã®è¨˜äº‹ä¸€è¦§ã‚’å–å¾—ä¸­...")
            url = f"https://note.com/api/v2/creators/{user_id}/contents?kind=note&page={page}"
            res = requests.get(url, timeout=10)
            res.raise_for_status()
            data = res.json()["data"]

            if not data["contents"]:
                break

            for content in data["contents"]:
                key = content.get("key")
                if key:
                    hashtags = [tag["hashtag"]["name"] for tag in content.get("hashtags", []) if "hashtag" in tag and tag.get("hashtag")]
                    title = content.get("name", "ç„¡é¡Œ")
                    all_notes[key] = {"hashtags": hashtags, "title": title}

            if data["isLastPage"]:
                break
            page += 1
            time.sleep(3)
        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼: è¨˜äº‹ä¸€è¦§ã®å–å¾—ä¸­ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ {e}")
            return None
    print("ã™ã¹ã¦ã®åŸºæœ¬æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
    return all_notes

def get_note_detail(note_key):
    """èªè¨¼æƒ…å ±ã‚’ä½¿ã£ã¦ã€æŒ‡å®šã•ã‚ŒãŸã‚­ãƒ¼ã®noteè¨˜äº‹è©³ç´°ã‚’å–å¾—ã—ã¾ã™ã€‚"""
    token1 = os.getenv("NOTE_GQL_AUTH_TOKEN")
    token2 = os.getenv("_NOTE_SESSION_V5")

    cookies = {}
    if token1 and token2:
        cookies['note_gql_auth_token'] = token1
        cookies['_note_session_v5'] = token2
    else:
        print("è­¦å‘Š: .envãƒ•ã‚¡ã‚¤ãƒ«ã«å¿…è¦ãªèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ç„¡æ–™éƒ¨åˆ†ã®ã¿å–å¾—ã—ã¾ã™ã€‚")

    try:
        url = f"https://note.com/api/v3/notes/{note_key}"
        res = requests.get(url, timeout=10, cookies=cookies)
        res.raise_for_status()
        return res.json()["data"]
    except Exception as e:
        print(f"    -> ã‚¨ãƒ©ãƒ¼: è¨˜äº‹è©³ç´°ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ {note_key}, {e}")
        return None

def save_as_markdown(note_key, note_info, all_notes_info, output_dir):
    """è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã€Markdownãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚"""
    try:
        note_detail = get_note_detail(note_key)
        if not note_detail:
            return

        title = note_detail.get("name", "ç„¡é¡Œ")
        eyecatch_url = note_detail.get("eyecatch")
        body_html = note_detail.get("body", "")
        
        hashtags_from_list = note_info.get("hashtags", [])
        hashtags_from_detail = [t["hashtag"]["name"] for t in note_detail.get("hashtag_notes", []) if "hashtag" in t and t.get("hashtag")]
        hashtags = sorted(list(set(hashtags_from_list + hashtags_from_detail)))

        soup = BeautifulSoup(body_html, "html.parser")
        
        replacements_after_md = []

        for i, figure in enumerate(soup.find_all("figure")):
            placeholder = f"<!-- EMBED_PLACEHOLDER_{i} -->"
            service = figure.get("embedded-service")
            final_markdown = ""

            if service == "note":
                related_key = figure.get("data-identifier")
                if not related_key or related_key == note_key: continue

                print(f"    -> noteãƒªãƒ³ã‚¯ç™ºè¦‹: {related_key}ã€‚è©³ç´°ã‚’å–å¾—ä¸­...")
                time.sleep(3)
                related_detail = get_note_detail(related_key)
                if related_detail:
                    original_related_title = all_notes_info.get(related_key, {}).get("title", related_detail.get("name", "ç„¡é¡Œ"))
                    safe_related_title = sanitize_filename(original_related_title)
                    internal_link_target = f"{related_key}_{safe_related_title}"
                    external_url = related_detail.get("note_url", "")
                    related_eyecatch_url = related_detail.get("eyecatch", "")

                    final_markdown = f"[[{internal_link_target}]][ğŸŒ]({external_url})"
                    if related_eyecatch_url:
                        final_markdown += f"\n\n![thumbnail]({related_eyecatch_url})\n"

            elif service == "twitter":
                print(f"    -> Twitterãƒã‚¹ãƒˆç™ºè¦‹ã€‚HTMLã‚’ä¿æŒã—ã¾ã™ã€‚")
                # figureã‚¿ã‚°ã®å†…å´ã«ã‚ã‚‹ div.twitter-tweet ã‚’å–å¾—
                tweet_div = figure.find("div", class_="twitter-tweet")
                if tweet_div:
                    final_markdown = str(tweet_div)

            elif service == "youtube":
                print(f"    -> YouTubeå‹•ç”»ç™ºè¦‹ã€‚ã‚µãƒ ãƒã‚¤ãƒ«ã¨ãƒªãƒ³ã‚¯ã«å¤‰æ›ã—ã¾ã™ã€‚")
                # styleå±æ€§ã‹ã‚‰ã‚µãƒ ãƒã‚¤ãƒ«URLã‚’æŠ½å‡º
                thumb_div = figure.find("div", class_="ytp-cued-thumbnail-overlay-image")
                if thumb_div and 'style' in thumb_div.attrs:
                    style_attr = thumb_div['style']
                    match = re.search(r'url\("(.*?)"\)', style_attr)
                    if match:
                        thumb_url = match.group(1)
                        video_url = figure.get("data-src", "")
                        final_markdown = f'<img src="{thumb_url}"><br>ï¼ˆ**URL:** [{video_url}]({video_url})ï¼‰'

            if final_markdown:
                figure.replace_with(placeholder)
                replacements_after_md.append((placeholder, final_markdown))

        h = html2text.HTML2Text()
        h.body_width = 0
        body_md = h.handle(str(soup))

        for placeholder, final_markdown in replacements_after_md:
            body_md = body_md.replace(placeholder, final_markdown)

        md_content = f"# {title}\n\n**URL:** {note_detail.get('note_url', '')}\n\n"
        if eyecatch_url:
            md_content += f"![eyecatch]({eyecatch_url})\n\n"
        md_content += "---\n\n"
        md_content += body_md

        if hashtags:
            md_content += "\n\n---\n\n## ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°\n"
            for tag in hashtags:
                md_content += f"- {tag}\n"

        safe_title = sanitize_filename(title)
        file_name = f"{note_key}_{safe_title}.md"
        file_path = os.path.join(output_dir, file_name)

        with open(file_path, "w", encoding="utf-8") as f:
            f.write(md_content)
        print(f"  -> å®Œäº†: {file_path}")

    except Exception as e:
        print(f"  -> ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ {note_key}, {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    print(f"è¨˜äº‹ã®ä¿å­˜å…ˆ: {os.path.abspath(OUTPUT_DIR)}")

    all_notes_info = get_all_notes_info(USER_ID)

    if not all_notes_info:
        print("è¨˜äº‹æƒ…å ±ãŒå–å¾—ã§ããªã‹ã£ãŸãŸã‚ã€å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        return

    # --- ãƒ†ã‚¹ãƒˆå¯¾è±¡ã®ã‚­ãƒ¼ã‚’ã“ã“ã«æŒ‡å®š ---
    note_keys_to_test = ["n3d8a5a1a332f"]
    print(f"\nâ˜…â˜…â˜… ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: {note_keys_to_test} ã®ã¿å–å¾—ã—ã¾ã™ â˜…â˜…â˜…\n")

    total_notes = len(note_keys_to_test)
    print(f"\nåˆè¨ˆ{total_notes}ä»¶ã®è¨˜äº‹ã‚’å‡¦ç†ã—ã¾ã™ã€‚")
    for i, note_key in enumerate(note_keys_to_test):
        print(f"({i + 1}/{total_notes}) è¨˜äº‹ã‚’å‡¦ç†ä¸­: {note_key}")
        note_info = all_notes_info.get(note_key, {})
        save_as_markdown(note_key, note_info, all_notes_info, OUTPUT_DIR)
        time.sleep(3)

    print("\nã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    main()